{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "#import cv2\n",
    "from PIL import Image \n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "https://www.oreilly.com/library/view/ai-and-machine/9781492078180/ch04.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 13:44:00.100005: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-10-29 13:44:00.100122: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#the first method\n",
    "(train_dataset, test_dataset), dataset_info = tfds.load('mnist',\n",
    "                                                        split=[\"train\",\"test\"],\n",
    "                                                        with_info=True,\n",
    "                                                        shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 13:44:00.135517: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-29 13:44:00.135717: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-10-29 13:44:00.157999: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sample in train_dataset.take(1): #as the take number increases (from 2 to 5), the last bbox related to the last images are shown. \n",
    "    image = sample[\"image\"]\n",
    "    label = sample[\"label\"]\n",
    "    print(label)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second method\n",
    "#/home/abaygi/tensorflow_datasets/mnist/3.0.1\n",
    "(train_dataset, test_dataset), dataset_info = tfds.load(\"mnist\", \n",
    "                                                        split=[\"train\", \"test\"], \n",
    "                                                        with_info=True, \n",
    "                                                        shuffle_files=True, \n",
    "                                                        batch_size=-1, \n",
    "                                                        as_supervised=True)\n",
    "\n",
    "#(training_images, training_labels), (test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images, training_labels = tfds.as_numpy(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images[30].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = tfds.as_numpy(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = tfds.load('mnist', split='train', shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29f0de370>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "In order to use the pre-trained model introduced later in the tutorial, we will need to convert each image into a size of at least (75, 75, 3). Due to the limited RAM, we will use the minimum size. Note that we also have to include three channels instead of just 1 (just copy the same channel 3 times) to make use of the pre-trained weights.\n",
    "\n",
    "https://www.kaggle.com/saumandas/intro-to-transfer-learning-with-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "\n",
    "def change_size(image):\n",
    "    img = array_to_img(image, scale=False) #returns PIL Image\n",
    "    img = img.resize((75, 75)) #resize image\n",
    "    img = img.convert(mode='RGB') #makes 3 channels\n",
    "    arr = img_to_array(img) #convert back to array\n",
    "    return arr.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_75 = [change_size(img) for img in training_images[:10000]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_75 = np.array(training_images_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29f206850>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_images_75[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Data augmentation is one of the most fundamental and useful strategy when dealing with images. It becomes especially useful if we have few images. This is not the case with this dataset, but it will still help anyway. Fortunately, ImageDataGenerator from keras makes it extremely simple to apply these augmentations. However, it is important to be careful when deciding which augmentations to apply.\n",
    "\n",
    "https://www.kaggle.com/saumandas/intro-to-transfer-learning-with-mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_gen = ImageDataGenerator(rescale=1./255, #easier for network to interpret numbers in range [0,1]\n",
    "                              zoom_range=0.1,\n",
    "                              width_shift_range=0.2,\n",
    "                              height_shift_range=0.2,\n",
    "                              validation_split=0.2) # 80/20 train/val split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(training_labels[:10000]) # one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 75, 75, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images_75.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = image_gen.flow(training_images_75, \n",
    "                                 y,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                subset='training',\n",
    "                                seed=42)\n",
    "valid_generator = image_gen.flow(training_images_75,\n",
    "                                 y,\n",
    "                                batch_size=16,\n",
    "                                shuffle=True,\n",
    "                                subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Many features in general images are common and it is not worth starting to train from scratch. Usually the beginning layers in convolutional neural networks identify extremely basic features such as vertical and horizontal lines. This is what forms the backbone of tranfer learning. Using other peoples models, which have been trained on lots of data and are capable of identifying simple features, to fit your own data.\n",
    "\n",
    "Imagenet\n",
    "\n",
    "Imagenet is one of the largest image databases in the world! Every year, they host a competition known as the Imagenet Large Scale Visual Recognition Challenge (ILSVRC), where scientists from all over the world compete to create the best model. A couple years back, a group of researchers came up with the ResNet50, and now it's available to everyone! Learn more about the model here: resnet50 info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape_size for ResNet50\n",
    "In practice, it is best to use this model with image size (224, 224, 3) since the original model was trained with that size. However, for the purpose of this tutorial (75, 75, 3) will also work. Note that we specified the weights as 'imagenet'. This will automatically load those pretrained weights into the model. If we do not specify, the weights will be initialized randomly and we would be starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#model transfer learing \n",
    "model.add(tf.keras.applications.resnet50.ResNet50(input_shape=(75, 75, 3),\n",
    "                                                 include_top = False, \n",
    "                                                 weights = \"imagenet\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abaygi/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x107e4b8e0>\n",
      "<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x29f25df10>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f27f2e0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29f27f460>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f27fac0>\n",
      "<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x29f1adfa0>\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x29f824b20>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f824940>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29f858a90>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f85d490>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f852df0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29dc719a0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f85dbe0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f8345b0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f1adc40>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29f852280>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29f85faf0>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x29f865c10>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f86b0a0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f872e20>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29f879af0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f85f430>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f87ebe0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29f82abb0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f824820>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f82aa60>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29faa3190>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x29f8874c0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29faa3070>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fab23d0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fab7160>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f82a6d0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29faa97f0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fac1220>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fac11f0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29faa9f10>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29facd670>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x29fad4790>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fad4dc0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fb25c10>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fb2b4c0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f872df0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fb39be0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fb3c760>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fb43880>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29facdaf0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fb436d0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fb256d0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fb49c40>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x29fb57070>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fb57bb0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fb433a0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fb30d60>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fad4610>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fb3c0a0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fabd9d0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f8878b0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fb30ca0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29f86baf0>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x29f858d00>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f85fa00>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29f8356d0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fb45490>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fb45f40>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fb5aa00>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fb5f550>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f8653a0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fab7f70>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a82f3970>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x29fb5c820>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a82fa5b0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a82ffc70>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a8305940>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a8305f40>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a82faee0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a8312a00>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a8305d90>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a8312fd0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a831eee0>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2a8317c70>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a83259d0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a8486fd0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a848e640>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fb43730>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a8494ca0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a84a1130>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a84a1fa0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a831e2b0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a8499eb0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a8481eb0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a8486ee0>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2a8317a90>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a830bb20>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a82fa760>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29f85f040>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fb3cca0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a82f00a0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x29fb57af0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29f87ea90>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a84a7b50>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a848fca0>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2a84abdc0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a84aeee0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a84aff70>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a84b2c70>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a84b6d90>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a84ae5b0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a84bad30>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a848f7c0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a84bab80>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a950c310>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2a950c040>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a84b6f10>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a9518550>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a951d2e0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a951d640>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a9511c10>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a952b3a0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a9511e50>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a9518be0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a95347f0>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2a953b910>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a953be20>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aad43af0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aad4a7c0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aad433d0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a953ba90>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aad54880>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aad5a9a0>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aad54730>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a95346d0>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2aad5a8b0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a951dc40>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a9507730>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a84afa90>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x29fb57b80>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x29fabdee0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2a831e040>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a82ff700>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a848ec40>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aad61580>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2a82fa7f0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aad63310>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2a831e6d0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aad6d850>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2a82fab20>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aad79be0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aad806d0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aad86730>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aad61640>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aad80ac0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aad6a520>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aad8daf0>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2aad92cd0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aad79a90>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aad98df0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aada1af0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aad86b80>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aada1940>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aadabb80>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aada1b50>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aadabac0>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aadbc130>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x2aadaeb50>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aadbc520>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aadcb370>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aadcbaf0>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aadd0f10>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aadbcf40>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aadab340>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aad80130>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2aadaed30>\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2aad6d310>\n",
      "<tensorflow.python.keras.layers.merge.Add object at 0x29fb57b50>\n",
      "<tensorflow.python.keras.layers.core.Activation object at 0x2aad61f70>\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[0].layers:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[0].layers:\n",
    "    if layer.name == 'conv5_block1_0_conv':\n",
    "        break\n",
    "    layer.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 13:44:11.933553: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 0s - loss: 0.4376 - accuracy: 0.8716"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 13:45:13.619194: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "250/250 [==============================] - 68s 263ms/step - loss: 0.4376 - accuracy: 0.8716 - val_loss: 2.6147 - val_accuracy: 0.1700\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                   validation_data=valid_generator, \n",
    "                   epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    " tf.keras.layers.Dense(128, activation='relu'),\n",
    " tf.keras.layers.Dropout(0.2),\n",
    " tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    " loss=loss_fn,\n",
    " metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
